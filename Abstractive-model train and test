#dataset is Xsum train data 1.csv
pip install transformers[torch] accelerate -U #requried insatllation
pip install datasets
import pandas as pd #pandas librariy
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
import torch
from sklearn.model_selection import train_test_split
from datasets import Dataset, load_metric

# Load the CSV file
file_path = '/mnt/data/Xsum train data 1.csv'
df = pd.read_csv('/content/drive/MyDrive/Xsum train data 1.csv')

# Display the first few rows of the DataFrame
print(df.head())

# Fill NaN values with empty strings
df.fillna("", inplace=True)
# Initialize the tokenizer
tokenizer = T5Tokenizer.from_pretrained("t5-small")

# Define the preprocess function
def preprocess_data(dataframe, source_text_column, target_text_column):
    inputs = ["summarize: " + str(doc) for doc in dataframe[source_text_column]]
    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding="max_length", return_tensors="pt")

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(dataframe[target_text_column].tolist(), max_length=128, truncation=True, padding="max_length", return_tensors="pt")

    model_inputs['labels'] = labels['input_ids']
    return model_inputs

# Apply the preprocessing to the DataFrame
processed_data = preprocess_data(df, 'document', 'summary')  # Adjust column names as needed

# Convert the processed data to dictionary
data_dict = {key: value.numpy() for key, value in processed_data.items()}

# Split the data
train_size = 0.9
train_dataset, test_dataset = train_test_split(df, train_size=train_size, random_state=42)

# Convert to datasets
train_dataset = Dataset.from_pandas(train_dataset)
test_dataset = Dataset.from_pandas(test_dataset)

# Tokenize datasets
def tokenize_function(examples):
    inputs = ["summarize: " + doc for doc in examples["document"]]
    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding="max_length")

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(examples["summary"], max_length=128, truncation=True, padding="max_length")

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

train_dataset = train_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

# Set format for PyTorch
train_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])
test_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])
! pip install datasets transformers accelerate 'transformers[torch]' -U huggingface_hub torch
from huggingface_hub import notebook_login
notebook_login()
import pandas as pd
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
import torch
from sklearn.model_selection import train_test_split
from datasets import Dataset, load_metric

# Load the CSV file
file_path = '/mnt/data/Xsum train data 1.csv'
df = pd.read_csv('/content/drive/MyDrive/Xsum train data 1.csv')

# Display the first few rows of the DataFrame
print(df.head())

# Fill NaN values with empty strings
df.fillna("", inplace=True)
# Initialize the tokenizer
tokenizer = T5Tokenizer.from_pretrained("t5-small")

# Define the preprocess function
def preprocess_data(dataframe, source_text_column, target_text_column):
    inputs = ["summarize: " + str(doc) for doc in dataframe[source_text_column]]
    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding="max_length", return_tensors="pt")

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(dataframe[target_text_column].tolist(), max_length=128, truncation=True, padding="max_length", return_tensors="pt")

    model_inputs['labels'] = labels['input_ids']
    return model_inputs

# Apply the preprocessing to the DataFrame
processed_data = preprocess_data(df, 'document', 'summary')  # Adjust column names as needed

# Convert the processed data to dictionary
data_dict = {key: value.numpy() for key, value in processed_data.items()}

# Split the data
train_size = 0.9
train_dataset, test_dataset = train_test_split(df, train_size=train_size, random_state=42)

# Convert to datasets
train_dataset = Dataset.from_pandas(train_dataset)
test_dataset = Dataset.from_pandas(test_dataset)

# Tokenize datasets
def tokenize_function(examples):
    inputs = ["summarize: " + doc for doc in examples["document"]]
    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding="max_length")

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(examples["summary"], max_length=128, truncation=True, padding="max_length")

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

train_dataset = train_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

# Set format for PyTorch
train_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])
test_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])
# Initialize the model
model = T5ForConditionalGeneration.from_pretrained("t5-small")

# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    save_total_limit=1,
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)

# Train the model
trainer.train()

# Save the model
model.save_pretrained("./abstractive_model")
tokenizer.save_pretrained("./abstractive_model")
#after three epoch the model trained well
"""Epoch	Training Loss	Validation Loss
1	0.696500	0.620079
2	0.673400	0.611563
3	0.659400	0.609784"""
pip install rouge_score
# Function to generate summaries and evaluate them
def evaluate(model, tokenizer, dataset, max_input_length=512, max_target_length=128):
    model.eval()
    metric = load_metric("rouge")
    summaries = []
    references = []

    for batch in dataset:
        input_ids = batch["input_ids"].unsqueeze(0).to(model.device)
        attention_mask = batch["attention_mask"].unsqueeze(0).to(model.device)

        with torch.no_grad():
            summary_ids = model.generate(input_ids, attention_mask=attention_mask, max_length=max_target_length)

        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        reference = tokenizer.decode(batch["labels"], skip_special_tokens=True)

        summaries.append(summary)
        references.append(reference)

    results = metric.compute(predictions=summaries, references=references, use_stemmer=True)
    return results

# Evaluate the model on the test data
results = evaluate(model, tokenizer, test_dataset)
print("ROUGE-1:", results["rouge1"].high.fmeasure)
print("ROUGE-2:", results["rouge2"].high.fmeasure)
print("ROUGE-L:", results["rougeL"].high.fmeasure)
#after evolution the ROUGH scores
"""ROUGE-1: 0.2897436474211089
ROUGE-2: 0.07612913769479013
ROUGE-L: 0.22019318663391835"""
